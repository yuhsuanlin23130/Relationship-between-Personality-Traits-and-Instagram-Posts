{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tutorial -         \n",
    "https://codertw.com/%E7%A8%8B%E5%BC%8F%E8%AA%9E%E8%A8%80/32296/         \n",
    "新增特殊字元to stopwords.txt             \n",
    "stopwords來源: https://github.com/tomlinNTUB/Python-in-5-days/blob/master/10-2%20%E4%B8%AD%E6%96%87%E6%96%B7%E8%A9%9E-%E7%A7%BB%E9%99%A4%E5%81%9C%E7%94%A8%E8%A9%9E.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16363636363636364, 0.4, 0.18181818181818182, 0.2545454545454545]\n"
     ]
    }
   ],
   "source": [
    "num_classes = 4\n",
    "\n",
    "num_DocsInClass = [9,22,10,14]  #training set\n",
    "\n",
    "num_AllDocs = sum(num_DocsInClass)\n",
    "\n",
    "prior_prob = []\n",
    "for i in range(num_classes):\n",
    "    prior_prob.append(num_DocsInClass[i] / num_AllDocs)\n",
    "print(prior_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract terms\n",
    "jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\aduser01\\Desktop\\IRTM\\Final Project\\NaiveBayes\\dict.txt.big.txt ...\n",
      "Loading model from cache C:\\Users\\aduser01\\AppData\\Local\\Temp\\jieba.u5d7072edb4d70e1372a6972f753c6c78.cache\n",
      "Loading model cost 0.981 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34265\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "import jieba\n",
    "import re\n",
    "import os\n",
    "import emoji\n",
    "\n",
    "'''userdict'''\n",
    "jieba.set_dictionary('./dict.txt.big.txt')\n",
    "jieba.load_userdict('./userdict.txt')\n",
    "\n",
    "'''stopword list'''\n",
    "stopword_list = []\n",
    "with open('stopwords.txt', 'r', encoding='UTF-8') as file:\n",
    "    for data in file.readlines():\n",
    "        data = data.strip()\n",
    "        stopword_list.append(data)\n",
    "\n",
    "#term\n",
    "dictionary = list()  #terms, df\n",
    "class_TFlist = list()  #(terms, tf in that doc)\n",
    "class_DFlist = list()  #(terms, tf in that doc)\n",
    "doc_tokens_list = []\n",
    "class_tokens_list = []\n",
    "N = num_AllDocs\n",
    "\n",
    "for classes in range(num_classes):  #num_classes, 4 \n",
    "    class_TFdict = dict()\n",
    "    class_DFdict = dict()\n",
    "    \n",
    "    \n",
    "    '''get the files under the directory'''''''''\n",
    "    for info in os.listdir(r'./data_posts/'+str(classes+1)): #1,2,3,4      \n",
    "        doc_has_word = list()  \n",
    "        post1 = pd.read_csv('data_posts/{}/{}'.format(str(classes+1),info), encoding=\"utf-8\", dtype=str) \n",
    "        \n",
    "        temp_tokens_list = []\n",
    "        # for each row\n",
    "        for i in range(post1.shape[0]):  \n",
    "            text = ''\n",
    "            try:\n",
    "                words = post1['post_caption'][i]\n",
    "                text = text + words + \" \"   \n",
    "            except:\n",
    "                None\n",
    "                           \n",
    "            # Remove emoji\n",
    "            text = emoji.demojize(text)\n",
    "            # Remove English words & numbers\n",
    "            #text = re.sub(\"[A-Za-z0-9\\!\\%\\[\\]\\,\\。\\-\\#\\?\\/]\", \"\", text) \n",
    "            text = re.sub(\"[A-Za-z0-9\\!\\%\\[\\]\\,\\。\\-\\#\\?\\/\\.\\_\\“\\”\\、]\", \"\", text) \n",
    "            # Remove the redundant spaces\n",
    "            text = re.sub(r'\\s\\s+',' ', text)   \n",
    "            # print(info, text)\n",
    "\n",
    "            '''Tokenize'''\n",
    "            tokens_list = jieba.lcut(text, cut_all=False)\n",
    "            # print(tokens_list)\n",
    "            \n",
    "            '''remove stopwords'''\n",
    "            indx1 = 0\n",
    "            while indx1 < len(tokens_list):\n",
    "                if tokens_list[indx1] in stopword_list or tokens_list[indx1]==' ' or tokens_list[indx1]=='\\n':\n",
    "                    del tokens_list[indx1]\n",
    "                else:\n",
    "                    if tokens_list[indx1] in class_TFdict:      #term freq of that class\n",
    "                        class_TFdict[tokens_list[indx1]] += 1\n",
    "                    else:\n",
    "                        class_TFdict[tokens_list[indx1]] = 1\n",
    "                        if tokens_list[indx1] not in dictionary:        \n",
    "                            dictionary.append(tokens_list[indx1]) \n",
    "                    if tokens_list[indx1] not in doc_has_word:\n",
    "                        if tokens_list[indx1] in class_DFdict:     #(term,一class中有幾篇文件有該term)    \n",
    "                            class_DFdict[tokens_list[indx1]] += 1\n",
    "                        else:\n",
    "                            class_DFdict[tokens_list[indx1]] = 1  \n",
    "                        doc_has_word.append(tokens_list[indx1])\n",
    "                    indx1 += 1\n",
    "            temp_tokens_list.extend(tokens_list)\n",
    "        doc_tokens_list.append(temp_tokens_list)\n",
    "    class_tokens_list.append(doc_tokens_list)     \n",
    "             \n",
    "    class_TFdict = sorted(class_TFdict.items(), key=lambda d: d[1], reverse = True)\n",
    "    class_TFlist.append(class_TFdict)     \n",
    "    class_DFdict = sorted(class_DFdict.items(), key=lambda d: d[1], reverse = True)       \n",
    "    class_DFlist.append(class_DFdict)   \n",
    "     \n",
    "        \n",
    "dictionary = sorted(dictionary) \n",
    "print(len(dictionary))\n",
    "print(len(class_TFlist))\n",
    "print(len(class_DFlist))            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176355\n"
     ]
    }
   ],
   "source": [
    "# number of tokens (以過濾掉stopwords)\n",
    "count = 0\n",
    "for i in range(52):  #\n",
    "    count += len(doc_tokens_list[i])\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('吃', 147), ('超', 137), ('最後', 128), ('其實', 123), ('走', 121), ('感覺', 116), ('不要', 113), ('現在', 111), ('照片', 109), ('時間', 107), ('第一次', 107), ('可愛', 104), ('開心', 102), ('快樂', 102), ('拍', 93), ('好像', 91), ('愛', 88), ('老師', 86), ('事', 81), ('應該', 77), ('事情', 76), ('更', 72), ('可能', 71), ('幫', 69), ('玩', 69), ('生活', 68), ('記得', 67), ('完', 66), ('加油', 65), ('前', 65), ('辛苦', 65), ('努力', 63), ('一定', 61), ('不能', 61), ('好吃', 61), ('地方', 60), ('朋友', 59), ('快', 58), ('好多', 58), ('超級', 58), ('每天', 58), ('聽', 58), ('笑', 57), ('一點', 57), ('整個', 57), ('蠻', 57), ('兩個', 56), ('日子', 56), ('問題', 55), ('竟然', 54), ('遇到', 54), ('特別', 54), ('嗚嗚', 53), ('表演', 53), ('比較', 52), ('完全', 52), ('有人', 51), ('跑', 50), ('哈哈哈', 50), ('每個', 49), ('時', 48), ('累', 48), ('結束', 47), ('補', 47), ('同學', 46), ('繼續', 46), ('能夠', 45), ('直接', 45), ('想要', 45), ('那種', 45), ('高中', 44), ('畢業', 44), ('明天', 43), ('根本', 43), ('認真', 42), ('人生', 42), ('小時', 42), ('世界', 41), ('－', 40), ('這張', 40), ('哭', 40), ('真', 39), ('耶', 39), ('話', 38), ('買', 38), ('新', 37), ('欸', 37), ('至少', 37), ('紀念', 36), ('永遠', 36), ('刺蝟', 36), ('久', 36), ('一堆', 36), ('聊天', 36), ('幸福', 36), ('溫暖', 36), ('期待', 36), ('穿', 35), ('這種', 35), ('最近', 35)]\n",
      "[('吃', 522), ('其實', 455), ('最後', 415), ('開心', 334), ('好像', 321), ('不要', 283), ('事', 280), ('現在', 272), ('感覺', 269), ('更', 265), ('時間', 248), ('加油', 246), ('照片', 242), ('完', 235), ('走', 232), ('第一次', 230), ('哈哈哈', 226), ('超', 221), ('跑', 215), ('一定', 210), ('可能', 210), ('大媒', 207), ('傳', 202), ('南四校', 197), ('快', 196), ('明天', 193), ('應該', 190), ('辛苦', 190), ('生活', 190), ('可愛', 181), ('前', 178), ('好多', 178), ('哭', 177), ('愛', 176), ('快樂', 175), ('玩', 169), ('繼續', 168), ('記得', 168), ('拍', 163), ('努力', 163), ('結束', 161), ('朋友', 161), ('每個', 158), ('笑', 153), ('讚', 152), ('嗚嗚', 151), ('幫', 146), ('兩個', 144), ('一點', 144), ('表演', 142), ('能夠', 138), ('突然', 137), ('日子', 137), ('畢業', 135), ('文', 135), ('不能', 134), ('謝謝你們', 131), ('事情', 131), ('活動', 127), ('聽', 124), ('期待', 123), ('一堆', 122), ('人生', 120), ('遇到', 119), ('睡', 118), ('真', 116), ('地方', 115), ('認真', 115), ('完全', 113), ('寫', 113), ('欸', 113), ('蠻', 112), ('想要', 112), ('時', 111), ('帶', 110), ('跳', 108), ('每天', 108), ('電影', 107), ('回家', 107), ('老師', 106), ('不到', 106), ('幸福', 106), ('超級', 104), ('直接', 104), ('不同', 101), ('話', 101), ('有人', 101), ('特別', 100), ('昨天', 100), ('比較', 99), ('變成', 99), ('未來', 98), ('台', 97), ('新', 97), ('請', 96), ('真是', 96), ('高中', 96), ('今年', 96), ('忘記', 96), ('禮拜', 95)]\n",
      "[('最後', 94), ('環島', 80), ('時', 74), ('其實', 62), ('事', 58), ('更', 52), ('吃', 52), ('第一次', 51), ('感覺', 46), ('照片', 46), ('現在', 45), ('跑', 42), ('不要', 42), ('可能', 39), ('應該', 37), ('完全', 37), ('騎', 37), ('活動', 36), ('前', 34), ('時間', 34), ('超', 34), ('決定', 33), ('開心', 33), ('結束', 32), ('老師', 32), ('每天', 31), ('玩', 31), ('參加', 31), ('生日', 30), ('地方', 30), ('學期', 29), ('完', 29), ('買', 29), ('走', 28), ('整個', 27), ('比較', 27), ('拍', 27), ('直接', 27), ('宿營', 27), ('遇到', 26), ('日子', 26), ('心情', 26), ('準備', 25), ('不能', 25), ('生活', 25), ('室友', 25), ('請', 25), ('聽', 24), ('不同', 24), ('表演', 24), ('公關', 24), ('一定', 23), ('小孩', 23), ('月', 23), ('站', 23), ('機會', 22), ('愛', 22), ('學長', 22), ('事情', 22), ('回憶', 22), ('帶', 22), ('快', 22), ('好像', 22), ('當初', 21), ('幾天', 21), ('未來', 20), ('出去玩', 20), ('兩個', 20), ('之夜', 20), ('實在', 20), ('機車', 20), ('沒想到', 19), ('特別', 19), ('一堆', 19), ('日本', 19), ('怕', 19), ('沖繩', 19), ('朋友', 18), ('蠻', 18), ('感受', 18), ('成為', 18), ('看起來', 18), ('鞋子', 18), ('行程', 18), ('民宿', 18), ('裡', 17), ('感到', 17), ('成功', 17), ('過程', 17), ('只能', 17), ('住', 17), ('嘉會', 17), ('突然', 17), ('文', 17), ('本來', 17), ('快樂', 17), ('還好', 17), ('天', 17), ('回來', 17), ('問題', 17)]\n",
      "[('最後', 66), ('電影', 56), ('吃', 54), ('更', 47), ('走', 46), ('可能', 46), ('開心', 44), ('時間', 44), ('現在', 43), ('感覺', 41), ('生活', 40), ('好像', 39), ('努力', 38), ('其實', 38), ('快樂', 34), ('完', 33), ('裡', 31), ('成為', 30), ('拍', 30), ('高中', 28), ('每個', 28), ('前', 28), ('地方', 28), ('導演', 28), ('事', 27), ('時', 27), ('不要', 27), ('世界', 27), ('特別', 27), ('每天', 26), ('老師', 25), ('期待', 24), ('一點', 23), ('好多', 23), ('台灣', 23), ('突然', 22), ('回憶', 22), ('一定', 22), ('超', 22), ('應該', 22), ('可愛', 21), ('不同', 21), ('事情', 21), ('只能', 20), ('未來', 20), ('時光', 20), ('結束', 20), ('這部', 20), ('非常', 20), ('幾天', 19), ('棒', 19), ('玩', 19), ('人生', 19), ('加油', 19), ('朋友', 19), ('社會', 19), ('值得', 19), ('愛', 19), ('寫', 19), ('第一次', 18), ('幸運', 18), ('認真', 18), ('想要', 18), ('不想', 18), ('照片', 18), ('美', 18), ('可惜', 18), ('畢業', 18), ('月', 18), ('重要', 18), ('瑪麗亞', 18), ('完成', 17), ('哭', 17), ('最好', 17), ('繼續', 17), ('博物館', 16), ('不能', 16), ('想念', 16), ('行程', 16), ('旅程', 16), ('站', 16), ('主人', 16), ('青春', 16), ('記得', 15), ('好吃', 15), ('外公', 15), ('明天', 15), ('辛苦', 15), ('整個', 15), ('文', 15), ('鳥蛋', 15), ('面對', 15), ('故事', 15), ('南京', 15), ('新', 14), ('幸福', 14), ('真是', 14), ('回到', 14), ('哈維爾', 14), ('魚肉', 14)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_classes):\n",
    "    print(class_TFlist[i][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''write dictionary to txt'''\n",
    "text_file = open( \"output_dictionary.txt\", \"w\")\n",
    "for indx in range(len(dictionary)): \n",
    "    try:\n",
    "        text_file.write(dictionary[indx] + '\\n')     # the actual doc_id   \n",
    "    except:\n",
    "        None\n",
    "text_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''過濾掉非中文字'''\n",
    "for i in range(1):    ######## num_classes\n",
    "    indx = 0\n",
    "    while indx < len(class_TFlist[i]):\n",
    "        if class_TFlist[i][indx][0] in dictionary[:389]:\n",
    "            del class_TFlist[i][indx]\n",
    "        else:\n",
    "            indx += 1\n",
    "    indx = 0\n",
    "    while indx < len(class_DFlist[i]):\n",
    "        if class_DFlist[i][indx][0] in dictionary[:389]:\n",
    "            del class_DFlist[i][indx]\n",
    "        else:\n",
    "            indx += 1\n",
    "            \n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('回來', 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_DFlist[0][57]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction (top 120 terms in each class)\n",
    "Likelihood Ratios     \n",
    "-2log(L(H1) / L(H2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31676868 0.15677551 0.15677551 ... 0.15677551 0.77961693 0.31676868]\n",
      "['太好', '茶', '回', '蛋糕', '普通', '網美', '獲得', '順帶', '拍出', '小型', '共識', '放手', '流星雨', '後院', '服儀', '有名', '縫', '酸奶', '貓貓', '澎湖', '談論', '歡迎', '便宜', '新', '推', '懶得', '隨便', '手機', '地獄', '背影', '星星', '床上', '滑', '居然', '搭', '學會', '這件', '揍', '組織', '地鐵', '計較', '受控', '外國人', '臺南', '木頭', '發生', '自然', '出門', '再度', '停', '泡', '早點', '愉快', '過了', '原來', '日期', '很會', '值', '抹', '優秀', '變好', '討厭', '發文', '早', '穿', '後悔', '瞎妹', '個位', '整得', '昏倒', '上升', '顆', '愛著', '衛生', '還加', '當了', '坦白', '臘腸狗', '頭還', '海水浴場', '禪寺', '頭才', '人給', '塌', '永和', '人想', '上船', '忍者', '還走', '放上', '呀呀', '搗蛋', '大得', '血檢', '缺一', '孬', '人體', '仙女', '更讓人', '點數', '蘿蔔絲', '畫個', '紀', '麻痺', '海島', '兩行', '禮服', '卡士', '面子', '碩', '七美', '多話', '瓶蓋', '蟬聯', '夠用', '豢養', '多顧慮', '珍重再見', '象山', '書籤', '八分', '夜貓', '豐盛', '夜行', '標的', '掩埋', '糖餅', '耳背', '紀律', '揶揄', '最快', '以此', '約翰', '緬甸', '杏仁', '教壞', '郊遊', '函', '教主', '第一支', '防備', '貴族', '升起', '潛水', '大吉', '燈塔', '伏特加', '安捏', '休息時間', '海市蜃樓']\n",
      "[ 0.03682469  0.44904836  0.80798221 ...  0.80798221 -0.\n",
      "  1.64125554]\n",
      "['還我', '服裝', '雀躍', '落幕', '即時', '油', '性感', '泰國', '班導', '堂', '辦得', '拔', '失誤', '還買', '跟上', '弟妹', '咻', '理智', '完好', '按摩', '嗨', '香蕉', '洗澡', '草', '山', '基本上', '走過', '下雨', '音樂', '嘴巴', '投票', '給了', '泡麵', '填滿', '貼心', '屁股', '接力', '好不好', '聽過', '超好', '做個', '盤子', '好想哭', '笑點', '會想', '委屈', '沒想', '不知所措', '運轉', '打著', '沒少', '標籤', '飽飽', '天冷', '失散多年', '充斥', '好快', '小女孩', '排斥', '澡', '難搞', '無感', '實作', '相聚', '決賽', '隊呼', '越來越少', '眼', '攝影棚', '小屁孩', '傳來', '腸胃炎', '鏡', '控', '抬', '輸出', '求救', '台北市', '帶隊', '給人', '盛夏', '秀', '松', '身處', '這屆', '量', '燈光', '叮嚀', '踴躍', '紅樓', '言', '白目', '好長', '掃', '前奏', '裡了', '馬尾', '發明', '發音', '置信', '重責大任', '知足', '四校', '滷味', '姐妹', '落後', '入鏡', '雙手', '甄', '芋圓', '貴人', '駐足', '外文', '差距', '純', '沙拉', '追尋', '說完', '良多', '包容', '狂', '發現自己', '掌聲', '場', '向前', '陰影', '反省', '樂', '破冰', '足跡', '免俗', '毅力', '偏好', '地鐵', '何其', '博物館', '道歉', '系烤', '抱', '助教', '跨年', '驚訝', '答應', '我家', '直屬', '很爽', '下台', '秘密', '支持', '複雜']\n",
      "[0.35581461 1.51773495 0.17607907 ... 0.17607907 0.91856848 0.35581461]\n",
      "['景象', '不夠', '複雜', '聽說', '好開心', '自動', '增長', '三星', '衰', '形形色色', '山腳', '少少的', '學時', '方塊', '三兩下', '衣褲', '大軍', '沒好', '地氣', '奮戰', '升級成', '田', '庫', '裝備', '泰勒', '緩步', '福利社', '終點站', '聖粉', '練個', '取名', '平地', '施工', '馬克', '篇章', '路途', '規矩', '擠出', '摩根', '會員', '一職', '步出', '行事', '報導', '區內', '疲累', '會點', '之情', '是夜', '陰陰的', '早早', '下午茶', '室', '剛上', '緊繃', '陌生', '小時候', '一點點', '事物', '莫名其妙', '瀏海', '吵', '重新', '偷偷', '早點', '文', '抽獎', '善良', '海', '哈哈哈哈', '女舞', '揪', '研究', '滑', '茶', '美國', '丟', '夠', '一路', '哥哥', '認同', '展', '聖誕節', '公主', '安慰', '砲', '共鳴', '超好', '奮鬥', '早就', '實際上', '雞', '城市', '成發', '團', '溫柔', '還想', '練舞', '榮幸', '群組', '責任', '社長', '放學', '要說', '鏡頭', '玩耍', '吹', '等待', '解釋', '睡著', '氣質', '最難', '登場', '和平', '重大', '段時間', '販賣機', '激昂', '園區', '試', '想起', '全世界', '震撼', '累到', '聽過', '溫度', '下課', '好不好', '天天', '貼心', '歷史', '變好', '優秀', '女人', '約會', '教育', '考完', '投票', '兩', '很帥', '考慮', '窩', '史', '猜', '翹', '提', '教學', '嘴巴', '舞台', '抹']\n",
      "[0.2499989  0.25789284 0.25789284 ... 0.25789284 0.03926879 0.52141676]\n",
      "['舒服', '穿', '進步', '遠', '表達', '錯', '室友', '盃', '隨便', '捷運', '真', '夜市', '聚', '打球', '當作', '暖', '歡迎', '目標', '日', '決定', '喝', '解決', '沒事', '擠', '惹', '學', '衣服', '腿', '放棄', '興奮', '合照', '跳', '故事', '舞', '搶', '幹話', '早點', '總覺', '自拍', '發現自己', '地上', '狂', '臭', '信心', '九份', '小隊', '還能', '願意', '心裡', '心情', '留下', '少', '順利', '好笑', '梗', '輔', '一日遊', '辣', '安心', '笑話', '早起', '這也', '耐心', '攝影師', '踩', '介紹', '緣分', '笑容', '生涯', '床上', '從沒', '茶', '抱怨', '成熟', '居然', '方面', '爸爸', '熬夜', '明顯', '濕', '心力', '冷氣', '肯定', '幫忙', '方式', '紀錄', '擔心', '幹嘛', '早', '表情', '飽', '對話', '一首歌', '猛', '鞋子', '左右', '兇', '咖啡', '懂得', '沒人', '人群', '烤', '模式', '奇妙', '好奇', '老人', '做過', '半夜', '失落', '隔壁', '傳', '跳舞', '張', '廁所', '便宜', '看著', '真正', '不錯', '意外', '慢慢', '旅行', '睡覺', '當初', '生活', '整個', '照片', '需要', '表演', '直接', '陪', '特別', '同學', '難', '出門', '自然', '蠻', '最大', '圖', '隔天', '嗨', '趕快', '拿到', '做到', '運動', '全身', '多多', '留言', '發個', '味道', '在意']\n",
      "572\n"
     ]
    }
   ],
   "source": [
    "''' build matrix '''\n",
    "appear_mtrx = np.zeros((len(dictionary),num_classes))  #class中有某term的文章數量\n",
    "\n",
    "for class_idx in range(num_classes):\n",
    "    for term_idx in range(len(class_DFlist[class_idx])): # 726 terms in class 1\n",
    "        indxxx = dictionary.index(class_DFlist[class_idx][term_idx][0])\n",
    "        appear_mtrx[indxxx][class_idx] = class_DFlist[class_idx][term_idx][1]  \n",
    "\n",
    "''' likelyhood ratios '''\n",
    "likelyhood_ratios = np.zeros((len(dictionary),num_classes))\n",
    "for term_idx in range(appear_mtrx.shape[0]): # all terms\n",
    "    for class_idx in range(num_classes):   \n",
    "        N11 = appear_mtrx[term_idx][class_idx]  #t在c中出現篇數\n",
    "        N10 = num_DocsInClass[class_idx] - N11\n",
    "        N01 = sum(appear_mtrx[term_idx][:]) - N11\n",
    "        N00 = num_AllDocs - num_DocsInClass[class_idx] - N01\n",
    "        # print(N11 + N01 + N00 + N10)\n",
    "        likelyhood_ratios[term_idx][class_idx] = -2*np.log10(np.power( (N11+N01)/num_AllDocs, N11 )*\n",
    "                                                             np.power( 1-(N11+N01)/num_AllDocs, N10 )*\n",
    "                                                             np.power( (N11+N01)/num_AllDocs, N01 )*\n",
    "                                                             np.power( 1-(N11+N01)/num_AllDocs, N00 )/\n",
    "                                                             np.power( N11/(N11+N10), N11 )/\n",
    "                                                             np.power( 1-N11/(N11+N10), N10 )/\n",
    "                                                             np.power( N01/(N01+N00), N01 )/\n",
    "                                                             np.power( 1-N01/(N01+N00), N00 ))\n",
    "#class_TFlist[0][150][0] # term\n",
    "#class_TFlist[0][150][1] # class' tf    \n",
    "\n",
    "''' Feature Selection for Multiple Classifiers '''\n",
    "'''全部一起選topk個'''\n",
    "#class_TFlist[0][150][0] # term\n",
    "#class_TFlist[0][150][1] # class' tf    \n",
    "\n",
    "#''' Feature Selection for Multiple Classifiers '''\n",
    "#mean_array = np.zeros(likelyhood_ratios.shape[0])\n",
    "#for i in range(likelyhood_ratios.shape[0]):\n",
    "#    mean_array[i] = np.mean(likelyhood_ratios[i][:])\n",
    "#\n",
    "#top_k = 150\n",
    "#top_k_idx = mean_array.argsort()[::-1][0:top_k]\n",
    "\n",
    "\n",
    "''' chi-square ratios '''\n",
    "chi_square_ratios = np.zeros((len(dictionary),num_classes))\n",
    "for term_idx in range(appear_mtrx.shape[0]): # all terms\n",
    "    for class_idx in range(num_classes):   \n",
    "        N11 = appear_mtrx[term_idx][class_idx]  #t在c中出現篇數\n",
    "        N10 = num_DocsInClass[class_idx] - N11\n",
    "        N01 = sum(appear_mtrx[term_idx][:]) - N11\n",
    "        N00 = num_AllDocs - num_DocsInClass[class_idx] - N01\n",
    "        N = N11+N10+N01+N00\n",
    "        present_ratio = (N11+N10)/N\n",
    "        on_topic_ratio = (N11+N01)/N\n",
    "        non_present_ratio = (N01+N00)/N\n",
    "        not_on_topic_ratio = (N10+N00)/N\n",
    "        exp1 = N * present_ratio * on_topic_ratio\n",
    "        exp2 = N * present_ratio * not_on_topic_ratio\n",
    "        exp3 = N * non_present_ratio * on_topic_ratio\n",
    "        exp4 = N * non_present_ratio * not_on_topic_ratio\n",
    "        chi_square_ratios[term_idx][class_idx] = np.power(N11-exp1,2)/exp1 + np.power(N10-exp2,2)/exp2 + np.power(N01-exp3,2)/exp3 + np.power(N00-exp4,2)/exp4\n",
    "# print(len(likelyhood_ratios))\n",
    "# print(len(chi_square_ratios),chi_square_ratios)\n",
    "\n",
    "\n",
    "\n",
    "'''each class 選topk個，再合併(remove duplicate)'''\n",
    "top_k = 150\n",
    "top_k_idx = []\n",
    "each_class_top = []\n",
    "for class_idx in range(num_classes):\n",
    "    mean_array = likelyhood_ratios[:,class_idx]\n",
    "    #mean_array = chi_square_ratios[:,class_idx]\n",
    "    print(mean_array)\n",
    "    class_top = mean_array.argsort()[::-1][0:top_k] \n",
    "    \n",
    "    #print(class_top[:20])\n",
    "    \n",
    "    temp_top = []\n",
    "    for origin_idx in class_top:\n",
    "        temp_top.append(dictionary[origin_idx])\n",
    "    print(temp_top)\n",
    "    each_class_top.append(temp_top)\n",
    "    \n",
    "    top_k_idx.extend( class_top )\n",
    "\n",
    "    \n",
    "top_k_idx = list(set(top_k_idx))    # remove duplicate\n",
    "print(len(top_k_idx))\n",
    "top_k = len(top_k_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['緩步', '從沒', '緬甸', '練個', '還想', '共識', '還我', '鞋子', '共鳴', '暖', '眼', '史', '多多', '練舞', '縫', '音樂', '氣質', '再度', '拿到', '睡著', '睡覺', '多話', '總覺', '心力', '順利', '順帶', '瞎妹', '多顧慮', '還能', '心情', '按摩', '夜市', '還買', '還走', '更讓人', '永和', '知足', '爸爸', '夜行', '夜貓', '合照', '求救', '夠', '心裡', '書籤', '對話', '夠用', '缺一', '走過', '蘿蔔絲', '人想', '決定', '決賽', '研究', '同學', '砲', '頭才', '破冰', '冷氣', '置信', '最大', '忍者', '捷運', '掃', '最快', '小型', '頭還', '小女孩', '掌聲', '人給', '沒事', '大吉', '顆', '沒人', '人群', '特別', '超好', '蛋糕', '小屁孩', '美國', '排斥', '碩', '沒好', '人體', '沒少', '願意', '小時候', '向前', '郊遊', '最難', '沒想', '會員', '大得', '介紹', '蟬聯', '狂', '會想', '出門', '一日遊', '群組', '接力', '吵', '函', '蠻', '越來越少', '吹', '仙女', '控', '猛', '血檢', '會點', '推', '性感', '呀呀', '猜', '翹', '行事', '社長', '小隊', '沙拉', '掩埋', '以此', '有名', '少', '趕快', '酸奶', '揍', '油', '少少的', '提', '老人', '味道', '衛生', '足跡', '獲得', '一職', '大軍', '飽', '伏特加', '飽飽', '和平', '衣服', '休息時間', '泡', '衣褲', '泡麵', '福利社', '一路', '咖啡', '跟上', '表情', '表演', '禪寺', '表達', '禮服', '衰', '考完', '秀', '揪', '重大', '咻', '考慮', '泰勒', '泰國', '屁股', '重新', '天冷', '揶揄', '居然', '一首歌', '天天', '一點點', '玩耍', '重責大任', '搗蛋', '哈哈哈哈', '展', '香蕉', '服儀', '跨年', '洗澡', '馬克', '馬尾', '秘密', '服裝', '哥哥', '耐心', '量', '七美', '珍重再見', '搭', '山', '班導', '路途', '跳', '耳背', '駐足', '三兩下', '何其', '山腳', '搶', '跳舞', '裝備', '木頭', '前奏', '太好', '想起', '摩根', '裡了', '惹', '聖粉', '踩', '流星雨', '踴躍', '愉快', '三星', '聖誕節', '聚', '意外', '剛上', '理智', '驚訝', '身處', '穿', '複雜', '左右', '海', '上升', '錯', '失散多年', '海島', '海市蜃樓', '善良', '海水浴場', '杏仁', '失落', '瓶蓋', '失誤', '甄', '窩', '聽說', '聽過', '差距', '奇妙', '擔心', '愛著', '喝', '便宜', '擠', '鏡', '擠出', '上船', '鏡頭', '輔', '要說', '下午茶', '下台', '松', '奮戰', '生活', '奮鬥', '生涯', '肯定', '女人', '輸出', '規矩', '攝影師', '攝影棚', '笑容', '背影', '信心', '笑話', '支持', '笑點', '下課', '助教', '慢慢', '女舞', '嗨', '下雨', '第一支', '帶隊', '好不好', '個位', '嘴巴', '田', '懂得', '辣', '好奇', '解決', '幫忙', '放上', '辦得', '解釋', '懶得', '值', '好快', '不夠', '平地', '放學', '好想哭', '言', '放手', '放棄', '包容', '留下', '好笑', '留言', '偏好', '計較', '討厭', '成熟', '故事', '成發', '做個', '做到', '好長', '好開心', '四校', '等待', '腸胃炎', '做過', '麻痺', '腿', '停', '幹嘛', '畫個', '答應', '幹話', '教主', '回', '追尋', '教壞', '教學', '防備', '梗', '區內', '教育', '我家', '當了', '臘腸狗', '當作', '溫度', '不知所措', '偷偷', '溫柔', '當初', '篇章', '姐妹', '自動', '整個', '試', '滑', '整得', '床上', '委屈', '自拍', '點數', '滷味', '自然', '這也', '這件', '傳', '陌生', '不錯', '傳來', '庫', '疲累', '這屆', '文', '認同', '臭', '廁所', '榮幸', '升級成', '升起', '陪', '丟', '園區', '陰影', '半夜', '樂', '陰陰的', '臺南', '圖', '團', '興奮', '糖餅', '標的', '標籤', '登場', '隊呼', '說完', '模式', '潛水', '系烤', '優秀', '發個', '舒服', '隔壁', '隔天', '澎湖', '紀', '新', '紀律', '澡', '舞', '隨便', '舞台', '紀錄', '激昂', '充斥', '發文', '發明', '約會', '博物館', '在意', '約翰', '濕', '兇', '地上', '弟妹', '發現自己', '發生', '手機', '卡士', '紅樓', '孬', '學', '張', '瀏海', '雀躍', '學時', '學會', '良多', '地氣', '發音', '地獄', '即時', '談論', '純', '白目', '芋圓', '方塊', '方式', '地鐵', '原來', '方面', '雙手', '施工', '免俗', '進步', '安心', '安慰', '安捏', '旅行', '累到', '雞', '坦白', '打球', '打著', '入鏡', '形形色色', '日', '之情', '烤', '城市', '完好', '歡迎', '日期', '難', '運動', '終點站', '早', '全世界', '基本上', '運轉', '難搞', '堂', '早就', '過了', '早早', '盃', '組織', '盛夏', '九份', '無感', '早起', '早點', '茶', '反省', '步出', '草', '報導', '盤子', '取名', '全身', '場', '兩', '變好', '目標', '給了', '受控', '給人', '歷史', '塌', '很帥', '明顯', '昏倒', '直屬', '直接', '室', '道歉', '室友', '莫名其妙', '星星', '投票', '填滿', '很會', '很爽', '相聚', '兩行', '遠', '段時間', '抬', '豐盛', '抱', '增長', '抱怨', '象山', '需要', '震撼', '照片', '豢養', '八分', '是夜', '事物', '抹', '毅力', '抽獎', '後悔', '公主', '貓貓', '落幕', '落後', '後院', '熬夜', '叮嚀', '網美', '販賣機', '實作', '看著', '責任', '貴人', '拍出', '貴族', '真', '實際上', '外國人', '緊繃', '燈光', '還加', '燈塔', '外文', '普通', '台北市', '緣分', '景象', '拔', '真正', '面子', '貼心']\n"
     ]
    }
   ],
   "source": [
    "#top_k_idx = sorted(top_k_idx)\n",
    "fea_extr_dictionary = []\n",
    "\n",
    "for origin_idx in top_k_idx:\n",
    "    fea_extr_dictionary.append(dictionary[origin_idx])\n",
    "    #fea_extr_dictionary.append((dictionary[origin_idx], origin_idx))  #原本的index\n",
    "print(fea_extr_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud \n",
    "#### 問卷資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['生活', '蠻', '特別', '表演', '同學', '直接', '真', '新', '穿', '需要', '故事', '文', '心情', '看著', '成發', '陪', '跳', '合照', '不錯', '回']\n",
      "['傳', '生活', '表演', '文', '真', '蠻', '跳', '直接', '特別', '新', '同學', '盃', '跳舞', '陪', '成發', '願意', '合照', '睡覺', '故事', '決定']\n",
      "['決定', '直接', '心情', '生活', '室友', '表演', '當初', '特別', '蠻', '鞋子', '文', '慢慢', '意外', '看著', '日', '這件', '新', '穿', '目標', '少']\n",
      "['生活', '特別', '博物館', '文', '故事', '新', '歷史', '想起', '表演', '城市', '包容', '展', '發生', '不夠', '同學', '地鐵', '旅行', '直接', '需要', '心情']\n"
     ]
    }
   ],
   "source": [
    "#for i in range(num_classes):\n",
    "#    print(class_TFlist[i][:100])\n",
    "\n",
    "def WordCloud(terms, saveplot=True, name=\"\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from wordcloud import WordCloud\n",
    "    wc = WordCloud(font_path='C:\\\\Windows\\\\Fonts\\\\mingliu.ttc', \n",
    "               background_color=\"black\",\n",
    "               prefer_horizontal=0.5,\n",
    "               max_words = 200)\n",
    "    plt.figure(dpi = 500)\n",
    "    wc.generate(\" \".join(terms))\n",
    "    plt.imshow(wc)\n",
    "    plt.axis(\"off\")\n",
    "    if(saveplot):\n",
    "        plt.savefig(name+\"_plot.jpg\")\n",
    "\n",
    "common = ['整個','照片']\n",
    "for i in range(num_classes):\n",
    "    lst = []\n",
    "    length = len(class_TFlist[i])\n",
    "    for j in range(length):\n",
    "        if class_TFlist[i][:][j][0] in fea_extr_dictionary and class_TFlist[i][:][j][0] not in common:   # in each_class_top[0]:\n",
    "            lst.append(class_TFlist[i][:][j][0])\n",
    "            #print(class_TFlist[i][:][j][0],class_TFlist[i][:][j][1])\n",
    "    print(lst[:20])\n",
    "    WordCloud(lst, name=\"class_{}\".format(str(i+1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Naive Bayes\n",
    "#### \"condprob\" matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "照片\n",
      "生活\n",
      "蠻\n",
      "整個\n",
      "特別\n",
      "表演\n",
      "同學\n",
      "直接\n",
      "真\n",
      "新\n",
      "穿\n",
      "需要\n",
      "文\n",
      "故事\n",
      "心情\n",
      "看著\n",
      "成發\n",
      "陪\n",
      "跳\n",
      "合照\n",
      "不錯\n",
      "願意\n",
      "回\n",
      "發生\n",
      "手機\n",
      "過了\n",
      "發文\n",
      "窩\n",
      "哈哈哈哈\n",
      "想起\n",
      "跳舞\n",
      "後悔\n",
      "慢慢\n",
      "進步\n",
      "搭\n",
      "決定\n",
      "舒服\n",
      "幹話\n",
      "原來\n",
      "隨便\n",
      "日\n",
      "學會\n",
      "吵\n",
      "幫忙\n",
      "夜市\n",
      "好開心\n",
      "惹\n",
      "便宜\n",
      "舞\n",
      "蛋糕\n",
      "幹嘛\n",
      "女舞\n",
      "擔心\n",
      "放棄\n",
      "真正\n",
      "半夜\n",
      "難\n",
      "攝影師\n",
      "旅行\n",
      "方式\n",
      "還能\n",
      "濕\n",
      "獲得\n",
      "衣服\n",
      "出門\n",
      "少\n",
      "討厭\n",
      "自然\n",
      "暖\n",
      "偷偷\n",
      "\n",
      "\n",
      "照片\n",
      "傳\n",
      "生活\n",
      "表演\n",
      "文\n",
      "真\n",
      "蠻\n",
      "跳\n",
      "直接\n",
      "特別\n",
      "新\n",
      "整個\n",
      "同學\n",
      "盃\n",
      "跳舞\n",
      "陪\n",
      "成發\n",
      "願意\n",
      "合照\n",
      "睡覺\n",
      "故事\n",
      "決定\n",
      "需要\n",
      "手機\n",
      "哈哈哈哈\n",
      "回\n",
      "幫忙\n",
      "當初\n",
      "喝\n",
      "搭\n",
      "打球\n",
      "看著\n",
      "日\n",
      "目標\n",
      "心情\n",
      "嗨\n",
      "舞\n",
      "擔心\n",
      "順利\n",
      "心裡\n",
      "幹嘛\n",
      "好開心\n",
      "穿\n",
      "討厭\n",
      "後悔\n",
      "小隊\n",
      "發生\n",
      "舞台\n",
      "原來\n",
      "支持\n",
      "幹話\n",
      "包容\n",
      "方式\n",
      "考完\n",
      "少\n",
      "發文\n",
      "衣服\n",
      "瀏海\n",
      "過了\n",
      "聖誕節\n",
      "不錯\n",
      "很爽\n",
      "走過\n",
      "真正\n",
      "學會\n",
      "最大\n",
      "趕快\n",
      "進步\n",
      "意外\n",
      "出門\n",
      "\n",
      "\n",
      "照片\n",
      "決定\n",
      "整個\n",
      "直接\n",
      "心情\n",
      "生活\n",
      "室友\n",
      "表演\n",
      "當初\n",
      "特別\n",
      "蠻\n",
      "鞋子\n",
      "文\n",
      "慢慢\n",
      "意外\n",
      "看著\n",
      "這件\n",
      "日\n",
      "穿\n",
      "新\n",
      "目標\n",
      "少\n",
      "合照\n",
      "拿到\n",
      "故事\n",
      "願意\n",
      "舒服\n",
      "真\n",
      "還能\n",
      "發生\n",
      "心裡\n",
      "手機\n",
      "隔天\n",
      "放棄\n",
      "順利\n",
      "不錯\n",
      "真正\n",
      "抽獎\n",
      "留下\n",
      "回\n",
      "隨便\n",
      "睡覺\n",
      "老人\n",
      "小隊\n",
      "哥哥\n",
      "最大\n",
      "同學\n",
      "紀錄\n",
      "自動\n",
      "對話\n",
      "幫忙\n",
      "臭\n",
      "搭\n",
      "總覺\n",
      "喝\n",
      "再度\n",
      "肯定\n",
      "包容\n",
      "跳\n",
      "介紹\n",
      "興奮\n",
      "衣服\n",
      "表達\n",
      "想起\n",
      "裝備\n",
      "這也\n",
      "旅行\n",
      "盃\n",
      "踩\n",
      "進步\n",
      "\n",
      "\n",
      "生活\n",
      "特別\n",
      "照片\n",
      "博物館\n",
      "故事\n",
      "文\n",
      "整個\n",
      "新\n",
      "歷史\n",
      "想起\n",
      "表演\n",
      "城市\n",
      "包容\n",
      "展\n",
      "發生\n",
      "地鐵\n",
      "不夠\n",
      "旅行\n",
      "同學\n",
      "直接\n",
      "需要\n",
      "心情\n",
      "美國\n",
      "討厭\n",
      "重新\n",
      "蠻\n",
      "一路\n",
      "願意\n",
      "小時候\n",
      "看著\n",
      "最大\n",
      "手機\n",
      "回\n",
      "陪\n",
      "過了\n",
      "慢慢\n",
      "好開心\n",
      "決定\n",
      "留下\n",
      "泡\n",
      "發文\n",
      "搭\n",
      "教育\n",
      "心裡\n",
      "偷偷\n",
      "夠\n",
      "當初\n",
      "複雜\n",
      "女人\n",
      "放棄\n",
      "走過\n",
      "音樂\n",
      "少\n",
      "要說\n",
      "原來\n",
      "鏡頭\n",
      "拿到\n",
      "真正\n",
      "山\n",
      "做到\n",
      "放學\n",
      "秘密\n",
      "優秀\n",
      "砲\n",
      "支持\n",
      "隔天\n",
      "哈哈哈哈\n",
      "日\n",
      "早就\n",
      "早\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "condprob = np.zeros((top_k, num_classes))\n",
    "top_condprob_lst = []\n",
    "\n",
    "for class_idx in range(num_classes):  \n",
    "    T_ct = list()  \n",
    "    termOfClass = list()\n",
    "    for temp_i in range(len(class_TFlist[class_idx])):\n",
    "        termOfClass.append(class_TFlist[class_idx][temp_i][0])\n",
    "        \n",
    "    for term in fea_extr_dictionary: # all terms\n",
    "        if term in termOfClass:\n",
    "            idxInClass = termOfClass.index(term)\n",
    "            T_ct.append( class_TFlist[class_idx][idxInClass][1])    \n",
    "        else:\n",
    "            T_ct.append(0)\n",
    "    # print(len(T_ct))\n",
    "    totalWordsInClass = np.sum(T_ct)\n",
    "    \n",
    "    for i in range(len(fea_extr_dictionary)): # all terms\n",
    "        condprob[i][class_idx] = (T_ct[i]+1)/(totalWordsInClass+top_k)\n",
    "        \n",
    "    top_condprob = condprob[:,class_idx].argsort()[::-1][:70] \n",
    "    top_condprob_lst.append(top_condprob)\n",
    "    for ttop in top_condprob:\n",
    "        print(fea_extr_dictionary[ttop])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc  1\n",
      "28\n",
      "8\n",
      "17\n",
      "48\n",
      "doc  2\n",
      "76\n",
      "2\n",
      "29\n",
      "82\n",
      "doc  3\n",
      "206\n",
      "55\n",
      "197\n",
      "727\n",
      "doc  4\n",
      "138\n",
      "28\n",
      "107\n",
      "267\n",
      "doc  5\n",
      "89\n",
      "8\n",
      "43\n",
      "112\n",
      "doc  6\n",
      "84\n",
      "11\n",
      "47\n",
      "150\n",
      "doc  7\n",
      "51\n",
      "3\n",
      "18\n",
      "70\n",
      "doc  8\n",
      "86\n",
      "6\n",
      "45\n",
      "168\n",
      "doc  9\n",
      "159\n",
      "34\n",
      "186\n",
      "484\n",
      "doc  10\n",
      "33\n",
      "86\n",
      "70\n",
      "287\n",
      "doc  11\n",
      "0\n",
      "1\n",
      "0\n",
      "6\n",
      "doc  12\n",
      "53\n",
      "80\n",
      "79\n",
      "289\n",
      "doc  13\n",
      "0\n",
      "0\n",
      "3\n",
      "2\n",
      "doc  14\n",
      "7\n",
      "13\n",
      "17\n",
      "25\n",
      "doc  15\n",
      "119\n",
      "124\n",
      "135\n",
      "383\n",
      "doc  16\n",
      "365\n",
      "440\n",
      "540\n",
      "1776\n",
      "doc  17\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "doc  18\n",
      "10\n",
      "29\n",
      "19\n",
      "62\n",
      "doc  19\n",
      "182\n",
      "166\n",
      "247\n",
      "803\n",
      "doc  20\n",
      "3\n",
      "15\n",
      "11\n",
      "50\n",
      "doc  21\n",
      "2\n",
      "9\n",
      "3\n",
      "13\n",
      "doc  22\n",
      "185\n",
      "202\n",
      "355\n",
      "822\n",
      "doc  23\n",
      "8\n",
      "21\n",
      "17\n",
      "65\n",
      "doc  24\n",
      "7\n",
      "16\n",
      "16\n",
      "36\n",
      "doc  25\n",
      "14\n",
      "28\n",
      "23\n",
      "91\n",
      "doc  26\n",
      "5\n",
      "7\n",
      "15\n",
      "22\n",
      "doc  27\n",
      "63\n",
      "100\n",
      "151\n",
      "335\n",
      "doc  28\n",
      "58\n",
      "113\n",
      "130\n",
      "248\n",
      "doc  29\n",
      "10\n",
      "16\n",
      "10\n",
      "32\n",
      "doc  30\n",
      "2\n",
      "22\n",
      "21\n",
      "83\n",
      "doc  31\n",
      "6\n",
      "17\n",
      "9\n",
      "45\n",
      "doc  32\n",
      "17\n",
      "6\n",
      "28\n",
      "102\n",
      "doc  33\n",
      "17\n",
      "8\n",
      "15\n",
      "89\n",
      "doc  34\n",
      "14\n",
      "9\n",
      "17\n",
      "80\n",
      "doc  35\n",
      "0\n",
      "0\n",
      "0\n",
      "6\n",
      "doc  36\n",
      "5\n",
      "4\n",
      "10\n",
      "65\n",
      "doc  37\n",
      "7\n",
      "0\n",
      "2\n",
      "9\n",
      "doc  38\n",
      "86\n",
      "27\n",
      "69\n",
      "425\n",
      "doc  39\n",
      "8\n",
      "3\n",
      "26\n",
      "116\n",
      "doc  40\n",
      "1\n",
      "0\n",
      "1\n",
      "12\n",
      "doc  41\n",
      "25\n",
      "4\n",
      "37\n",
      "115\n",
      "doc  42\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "doc  43\n",
      "0\n",
      "2\n",
      "5\n",
      "0\n",
      "doc  44\n",
      "11\n",
      "4\n",
      "36\n",
      "38\n",
      "doc  45\n",
      "2\n",
      "2\n",
      "7\n",
      "12\n",
      "doc  46\n",
      "20\n",
      "8\n",
      "27\n",
      "14\n",
      "doc  47\n",
      "3\n",
      "4\n",
      "5\n",
      "8\n",
      "doc  48\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "doc  49\n",
      "14\n",
      "13\n",
      "41\n",
      "36\n",
      "doc  50\n",
      "0\n",
      "1\n",
      "3\n",
      "0\n",
      "doc  51\n",
      "54\n",
      "48\n",
      "96\n",
      "164\n",
      "doc  52\n",
      "5\n",
      "1\n",
      "13\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "'''每篇文件中出現幾次(含重複) each_class_top 字詞'''\n",
    "for i, each_doc in enumerate(doc_tokens_list):\n",
    "    print(\"doc  \"+str(i+1))\n",
    "    count = 0\n",
    "    for word in each_doc:\n",
    "        if word in each_class_top[0]:\n",
    "            count += 1\n",
    "    print(count)\n",
    "    count = 0\n",
    "    for word in each_doc:\n",
    "        if word in each_class_top[1]:\n",
    "            count += 1\n",
    "    print(count)\n",
    "    count = 0\n",
    "    for word in each_doc:\n",
    "        if word in each_class_top[2]:\n",
    "            count += 1\n",
    "    print(count)\n",
    "    count = 0\n",
    "    for word in each_doc:\n",
    "        if word in each_class_top[3]:\n",
    "            count += 1\n",
    "    print(count)\n",
    "# num_DocsInClass = [9,22,12,15]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "for info in os.listdir(r'./validation_posts/'): #info\n",
    "    tokens = list()                   ################\n",
    "    score = np.zeros(num_classes)\n",
    "    post1 = pd.read_csv('validation_posts/{}'.format(info), encoding=\"utf-8\", dtype=str)        \n",
    "    # for each row\n",
    "    for i in range(post1.shape[0]):  \n",
    "        text = ''\n",
    "        try:\n",
    "            words = post1['post_caption'][i]\n",
    "            text = text + words + \" \"   \n",
    "        except:\n",
    "            None\n",
    "        text = emoji.demojize(text)\n",
    "        text = re.sub(\"[A-Za-z0-9\\!\\%\\[\\]\\,\\。\\-\\#\\?\\/\\.\\_\\“\\”\\、]\", \"\", text)\n",
    "        text = re.sub(r'\\s\\s+',' ', text)   \n",
    "        tokens_list = jieba.lcut(text, cut_all=False)\n",
    "        indx1 = 0\n",
    "        for word in tokens_list:\n",
    "            if word not in stopword_list or tokens_list[indx1]!=' ':\n",
    "                tokens.append(word)  \n",
    "\n",
    "    '''classification'''\n",
    "    # 4 classes, count 4 probabilities P(c|d) ~> P(c)*P(d|c)\n",
    "    for classes in range(num_classes):\n",
    "        score[classes] = np.log10(prior_prob[classes])\n",
    "        for word in tokens: \n",
    "            if word in fea_extr_dictionary:     #else:ignore\n",
    "                idxx = fea_extr_dictionary.index(word)\n",
    "                score[classes] += np.log10(condprob[idxx][classes]) \n",
    "\n",
    "    # print(score)\n",
    "    predict_class = np.argmax(score)\n",
    "    # print('predicted class ID of doc.', doc_id,':', predict_class + 1)  #ID: 1~13\n",
    "\n",
    "    with open('validation_output.csv', 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)          # 建立CSV檔寫入器\n",
    "        writer.writerow( [info[:-4], info[:1], predict_class+1] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7595238095238096 0.7916666666666666 0.7499999999999999\n",
      "0.8374999999999999 0.9 0.8333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "post = pd.read_csv('validation compare.csv', encoding=\"utf-8\", dtype=str)        \n",
    "y_true = post['classification']    \n",
    "y_pred1 = post['chi-square']    \n",
    "y_pred2 = post['likelyhood']    \n",
    "\n",
    "#chi-square\n",
    "f1 = f1_score( y_true, y_pred1, average='macro' )\n",
    "p = precision_score(y_true, y_pred1, average='macro')\n",
    "r = recall_score(y_true, y_pred1, average='macro')\n",
    "print(f1, p, r)\n",
    "\n",
    "#likelyhood\n",
    "f1 = f1_score( y_true, y_pred2, average='macro' )\n",
    "p = precision_score(y_true, y_pred2, average='macro')\n",
    "r = recall_score(y_true, y_pred2, average='macro')\n",
    "print(f1, p, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "wordDict = dict()\n",
    "''' Extract Tokens From Doc'''   \n",
    "# get the files under the directory\n",
    "test_cat = ['athlete','celebrity','politician','writer','youtuber']\n",
    "for cat in test_cat:\n",
    "    for info in os.listdir(r'./test_posts/{}/'.format(cat)): #info\n",
    "        tokens = list()                   ################\n",
    "        score = np.zeros(num_classes)\n",
    "        post1 = pd.read_csv('test_posts/{}/{}'.format(cat,info), encoding=\"utf-8\", dtype=str)        \n",
    "        # for each row\n",
    "        for i in range(post1.shape[0]):  \n",
    "            text = ''\n",
    "            try:\n",
    "                words = post1['post_caption'][i]\n",
    "                text = text + words + \" \"   \n",
    "            except:\n",
    "                None\n",
    "            text = emoji.demojize(text)\n",
    "            text = re.sub(\"[A-Za-z0-9\\!\\%\\[\\]\\,\\。\\-\\#\\?\\/\\.\\_\\“\\”\\、]\", \"\", text)\n",
    "            text = re.sub(r'\\s\\s+',' ', text)   \n",
    "            tokens_list = jieba.lcut(text, cut_all=False)\n",
    "            indx1 = 0\n",
    "            for word in tokens_list:\n",
    "                if word not in stopword_list or tokens_list[indx1]!=' ':\n",
    "                    tokens.append(word)  \n",
    "          \n",
    "            # tokens_list中包含top_condprob_lst[class_index]的字詞 &次數\n",
    "            for wordInx in top_condprob_lst[0]:  #第一類別\n",
    "                if fea_extr_dictionary[wordInx] in tokens_list:\n",
    "                    if fea_extr_dictionary[wordInx] not in wordDict:\n",
    "                        wordDict[fea_extr_dictionary[wordInx]] = 1\n",
    "                    else:\n",
    "                        wordDict[fea_extr_dictionary[wordInx]] += 1\n",
    "\n",
    "        '''classification'''\n",
    "        # 4 classes, count 4 probabilities P(c|d) ~> P(c)*P(d|c)\n",
    "        for classes in range(num_classes):\n",
    "            score[classes] = np.log10(prior_prob[classes])\n",
    "            for word in tokens: \n",
    "                if word in fea_extr_dictionary:     #else:ignore\n",
    "                    idxx = fea_extr_dictionary.index(word)\n",
    "                    score[classes] += np.log10(condprob[idxx][classes]) \n",
    "\n",
    "        # print(score)\n",
    "        predict_class = np.argmax(score)\n",
    "        # print('predicted class ID of doc.', doc_id,':', predict_class + 1)  #ID: 1~13\n",
    "\n",
    "        with open('test_output.csv', 'a', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)          # 建立CSV檔寫入器\n",
    "            writer.writerow( [info[:-4], predict_class+1] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'願意': 433,\n",
       " '新': 809,\n",
       " '衣服': 212,\n",
       " '生活': 803,\n",
       " '放棄': 229,\n",
       " '真正': 342,\n",
       " '決定': 349,\n",
       " '故事': 589,\n",
       " '心情': 280,\n",
       " '舒服': 123,\n",
       " '出門': 170,\n",
       " '討厭': 135,\n",
       " '需要': 696,\n",
       " '直接': 404,\n",
       " '幫忙': 172,\n",
       " '方式': 308,\n",
       " '難': 139,\n",
       " '蠻': 135,\n",
       " '發生': 360,\n",
       " '日': 509,\n",
       " '進步': 101,\n",
       " '還能': 161,\n",
       " '整個': 275,\n",
       " '陪': 395,\n",
       " '原來': 324,\n",
       " '慢慢': 225,\n",
       " '少': 156,\n",
       " '穿': 324,\n",
       " '學會': 194,\n",
       " '旅行': 215,\n",
       " '真': 404,\n",
       " '蛋糕': 86,\n",
       " '手機': 264,\n",
       " '惹': 73,\n",
       " '自然': 101,\n",
       " '回': 280,\n",
       " '獲得': 171,\n",
       " '攝影師': 77,\n",
       " '不錯': 166,\n",
       " '表演': 103,\n",
       " '同學': 205,\n",
       " '搭': 219,\n",
       " '隨便': 75,\n",
       " '特別': 543,\n",
       " '合照': 161,\n",
       " '照片': 566,\n",
       " '看著': 432,\n",
       " '幹嘛': 161,\n",
       " '偷偷': 168,\n",
       " '好開心': 92,\n",
       " '便宜': 41,\n",
       " '哈哈哈哈': 116,\n",
       " '發文': 82,\n",
       " '吵': 38,\n",
       " '後悔': 66,\n",
       " '過了': 112,\n",
       " '半夜': 54,\n",
       " '濕': 53,\n",
       " '擔心': 202,\n",
       " '想起': 367,\n",
       " '跳舞': 24,\n",
       " '跳': 125,\n",
       " '暖': 61,\n",
       " '文': 118,\n",
       " '舞': 16,\n",
       " '夜市': 45,\n",
       " '窩': 35,\n",
       " '幹話': 15,\n",
       " '成發': 2}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
